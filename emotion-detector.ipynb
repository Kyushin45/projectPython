{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","#for dirname, _, filenames in os.walk('/kaggle/input'):\n","    #for filename in filenames:\n","        #print(os.path.join(dirname, filename))\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","import keras\n","from keras.preprocessing import image\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import cv2\n","from tensorflow.keras.applications import VGG16, InceptionResNetV2\n","from keras import regularizers\n","from tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["train_dir = \"train\" #passing the path with training images\n","test_dir = \"test\"   #passing the path with testing images"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["img_size = 48 #original size of the image"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Data Augmentation\n","--------------------------\n","rotation_range = rotates the image with the amount of degrees we provide\n","width_shift_range = shifts the image randomly to the right or left along the width of the image\n","height_shift range = shifts image randomly to up or below along the height of the image\n","horizontal_flip = flips the image horizontally\n","rescale = to scale down the pizel values in our image between 0 and 1\n","zoom_range = applies random zoom to our object\n","validation_split = reserves some images to be used for validation purpose\n","\"\"\"\n","\n","train_datagen = ImageDataGenerator(#rotation_range = 180,\n","                                         width_shift_range = 0.1,\n","                                         height_shift_range = 0.1,\n","                                         horizontal_flip = True,\n","                                         rescale = 1./255,\n","                                         #zoom_range = 0.2,\n","                                         validation_split = 0.2\n","                                        )\n","validation_datagen = ImageDataGenerator(rescale = 1./255,\n","                                         validation_split = 0.2)"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 22968 images belonging to 7 classes.\n","Found 1432 images belonging to 7 classes.\n"]}],"source":["\"\"\"\n","Applying data augmentation to the images as we read \n","them from their respectivve directories\n","\"\"\"\n","train_generator = train_datagen.flow_from_directory(directory = train_dir,\n","                                                    target_size = (img_size,img_size),\n","                                                    batch_size = 64,\n","                                                    color_mode = \"grayscale\",\n","                                                    class_mode = \"categorical\",\n","                                                    subset = \"training\"\n","                                                   )\n","validation_generator = validation_datagen.flow_from_directory( directory = test_dir,\n","                                                              target_size = (img_size,img_size),\n","                                                              batch_size = 64,\n","                                                              color_mode = \"grayscale\",\n","                                                              class_mode = \"categorical\",\n","                                                              subset = \"validation\"\n","                                                             )"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["\"\\nModeling\\n\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(filters = 64,kernel_size = (3,3),padding = 'same',activation = 'relu',input_shape=(img_size,img_size,1)))\\nmodel.add(MaxPool2D(pool_size = 2,strides = 2))\\nmodel.add(BatchNormalization())\\n\\nmodel.add(Conv2D(filters = 128,kernel_size = (3,3),padding = 'same',activation = 'relu'))\\nmodel.add(MaxPool2D(pool_size = 2,strides = 2))\\nmodel.add(BatchNormalization())\\nmodel.add(Dropout(0.25))\\n\\nmodel.add(Conv2D(filters = 128,kernel_size = (3,3),padding = 'same',activation = 'relu'))\\nmodel.add(MaxPool2D(pool_size = 2,strides = 2))\\nmodel.add(BatchNormalization())\\nmodel.add(Dropout(0.25))\\n\\nmodel.add(Conv2D(filters = 256,kernel_size = (3,3),padding = 'same',activation = 'relu'))\\nmodel.add(MaxPool2D(pool_size = 2,strides = 2))\\nmodel.add(BatchNormalization())\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(units = 128,activation = 'relu',kernel_initializer='he_normal'))\\nmodel.add(Dropout(0.25))\\nmodel.add(Dense(units = 64,activation = 'relu',kernel_initializer='he_normal'))\\nmodel.add(BatchNormalization())\\nmodel.add(Dropout(0.25))\\nmodel.add(Dense(units = 32,activation = 'relu',kernel_initializer='he_normal'))\\nmodel.add(Dense(7,activation = 'softmax'))\\n\\n\""]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","Modeling\n","\n","\n","model = Sequential()\n","model.add(Conv2D(filters = 64,kernel_size = (3,3),padding = 'same',activation = 'relu',input_shape=(img_size,img_size,1)))\n","model.add(MaxPool2D(pool_size = 2,strides = 2))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(filters = 128,kernel_size = (3,3),padding = 'same',activation = 'relu'))\n","model.add(MaxPool2D(pool_size = 2,strides = 2))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(filters = 128,kernel_size = (3,3),padding = 'same',activation = 'relu'))\n","model.add(MaxPool2D(pool_size = 2,strides = 2))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(filters = 256,kernel_size = (3,3),padding = 'same',activation = 'relu'))\n","model.add(MaxPool2D(pool_size = 2,strides = 2))\n","model.add(BatchNormalization())\n","\n","model.add(Flatten())\n","model.add(Dense(units = 128,activation = 'relu',kernel_initializer='he_normal'))\n","model.add(Dropout(0.25))\n","model.add(Dense(units = 64,activation = 'relu',kernel_initializer='he_normal'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","model.add(Dense(units = 32,activation = 'relu',kernel_initializer='he_normal'))\n","model.add(Dense(7,activation = 'softmax'))\n","\n","\"\"\""]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]}],"source":["model= tf.keras.models.Sequential()\n","model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))\n","model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(128,(5,5), padding='same', activation='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","    \n","model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten()) \n","model.add(Dense(256,activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","    \n","model.add(Dense(512,activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","\n","model.add(Dense(7, activation='softmax'))\n","\n","model.compile(\n","    optimizer = Adam(lr=0.0001), \n","    loss='categorical_crossentropy', \n","    metrics=['accuracy']\n","  )"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["epochs = 60\n","batch_size = 64"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 48, 48, 32)        320       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 48, 48, 64)        18496     \n","                                                                 \n"," batch_normalization (Batch  (None, 48, 48, 64)        256       \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 24, 24, 64)        0         \n"," D)                                                              \n","                                                                 \n"," dropout (Dropout)           (None, 24, 24, 64)        0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 24, 24, 128)       204928    \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 24, 24, 128)       512       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 12, 12, 128)       0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_1 (Dropout)         (None, 12, 12, 128)       0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 12, 12, 512)       590336    \n","                                                                 \n"," batch_normalization_2 (Bat  (None, 12, 12, 512)       2048      \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_2 (MaxPoolin  (None, 6, 6, 512)         0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_2 (Dropout)         (None, 6, 6, 512)         0         \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 6, 6, 512)         2359808   \n","                                                                 \n"," batch_normalization_3 (Bat  (None, 6, 6, 512)         2048      \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_3 (MaxPoolin  (None, 3, 3, 512)         0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_3 (Dropout)         (None, 3, 3, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 4608)              0         \n","                                                                 \n"," dense (Dense)               (None, 256)               1179904   \n","                                                                 \n"," batch_normalization_4 (Bat  (None, 256)               1024      \n"," chNormalization)                                                \n","                                                                 \n"," dropout_4 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 512)               131584    \n","                                                                 \n"," batch_normalization_5 (Bat  (None, 512)               2048      \n"," chNormalization)                                                \n","                                                                 \n"," dropout_5 (Dropout)         (None, 512)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 7)                 3591      \n","                                                                 \n","=================================================================\n","Total params: 4496903 (17.15 MB)\n","Trainable params: 4492935 (17.14 MB)\n","Non-trainable params: 3968 (15.50 KB)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'scipy' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\DIPA\\Downloads\\archive (2)\\emotion-detector.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DIPA/Downloads/archive%20%282%29/emotion-detector.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# import scipy\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DIPA/Downloads/archive%20%282%29/emotion-detector.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# from scipy import optimize,tra\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DIPA/Downloads/archive%20%282%29/emotion-detector.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DIPA/Downloads/archive%20%282%29/emotion-detector.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x \u001b[39m=\u001b[39;49m train_generator,epochs \u001b[39m=\u001b[39;49m epochs,validation_data \u001b[39m=\u001b[39;49m validation_generator)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\preprocessing\\image.py:2526\u001b[0m, in \u001b[0;36mapply_affine_transform\u001b[1;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[0;32m   2482\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.preprocessing.image.apply_affine_transform\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_affine_transform\u001b[39m(\n\u001b[0;32m   2484\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2496\u001b[0m     order\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   2497\u001b[0m ):\n\u001b[0;32m   2498\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Applies an affine transformation specified by the parameters given.\u001b[39;00m\n\u001b[0;32m   2499\u001b[0m \n\u001b[0;32m   2500\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2524\u001b[0m \u001b[39m        ImportError: if SciPy is not available.\u001b[39;00m\n\u001b[0;32m   2525\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2526\u001b[0m     \u001b[39mif\u001b[39;00m scipy \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2527\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage transformations require SciPy. Install SciPy.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2529\u001b[0m     \u001b[39m# Input sanity checks:\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m     \u001b[39m# 1. x must 2D image with one or more channels (i.e., a 3D tensor)\u001b[39;00m\n\u001b[0;32m   2531\u001b[0m     \u001b[39m# 2. channels must be either first or last dimension\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'scipy' is not defined"]}],"source":["# import scipy\n","# from scipy import optimize,tra\n","import scipy\n","history = model.fit(x = train_generator,epochs = epochs,validation_data = validation_generator)"]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'history' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\DIPA\\Downloads\\archive (2)\\emotion-detector.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DIPA/Downloads/archive%20%282%29/emotion-detector.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fig , ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DIPA/Downloads/archive%20%282%29/emotion-detector.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_acc \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DIPA/Downloads/archive%20%282%29/emotion-detector.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_loss \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DIPA/Downloads/archive%20%282%29/emotion-detector.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m fig\u001b[39m.\u001b[39mset_size_inches(\u001b[39m12\u001b[39m,\u001b[39m4\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["fig , ax = plt.subplots(1,2)\n","train_acc = history.history['accuracy']\n","train_loss = history.history['loss']\n","fig.set_size_inches(12,4)\n","\n","ax[0].plot(history.history['accuracy'])\n","ax[0].plot(history.history['val_accuracy'])\n","ax[0].set_title('Training Accuracy vs Validation Accuracy')\n","ax[0].set_ylabel('Accuracy')\n","ax[0].set_xlabel('Epoch')\n","ax[0].legend(['Train', 'Validation'], loc='upper left')\n","\n","ax[1].plot(history.history['loss'])\n","ax[1].plot(history.history['val_loss'])\n","ax[1].set_title('Training Loss vs Validation Loss')\n","ax[1].set_ylabel('Loss')\n","ax[1].set_xlabel('Epoch')\n","ax[1].legend(['Train', 'Validation'], loc='upper left')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.save('model_optimal.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img = image.load_img(\"../input/emotion-detection-fer/test/happy/im1021.png\",target_size = (48,48),color_mode = \"grayscale\")\n","img = np.array(img)\n","plt.imshow(img)\n","print(img.shape) #prints (48,48) that is the shape of our image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["label_dict = {0:'Angry',1:'Disgust',2:'Fear',3:'Happy',4:'Neutral',5:'Sad',6:'Surprise'}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img = np.expand_dims(img,axis = 0) #makes image shape (1,48,48)\n","img = img.reshape(1,48,48,1)\n","result = model.predict(img)\n","result = list(result[0])\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img_index = result.index(max(result))\n","print(label_dict[img_index])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_loss, train_acc = model.evaluate(train_generator)\n","test_loss, test_acc   = model.evaluate(validation_generator)\n","print(\"final train accuracy = {:.2f} , validation accuracy = {:.2f}\".format(train_acc*100, test_acc*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.save_weights('model_weights.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
